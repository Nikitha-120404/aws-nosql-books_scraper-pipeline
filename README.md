ğŸ“š AWS NoSQL Books Data Pipeline

(Web Scraping â†’ Amazon DocumentDB â†’ Transformation â†’ Amazon DynamoDB)

ğŸ“Œ Project Overview

This project implements an end-to-end AWS NoSQL data pipeline using Python.
Book data is scraped from a public website, stored as raw documents in Amazon DocumentDB, transformed and cleaned, and then loaded into Amazon DynamoDB for scalable NoSQL storage and querying.

The pipeline reflects a real-world data engineering workflow executed via terminal (PuTTY/SSH) in an AWS environment.

ğŸ¯ Key Features

Web scraping of book metadata using Python

Raw data storage in Amazon DocumentDB (MongoDB-compatible)

Data cleaning and transformation

Structured data ingestion into Amazon DynamoDB

Terminal-based validation and querying

Secure TLS-based database connections

ğŸ›  Tech Stack

Programming Language: Python

Web Scraping: Requests, BeautifulSoup

AWS Services:

Amazon DocumentDB

Amazon DynamoDB

AWS SDK: AWS SDK for Python (boto3)

Database Client: pymongo

Infrastructure Access: PuTTY / SSH

Data Format: JSON

ğŸ”„ Data Pipeline Workflow

Scrape book data from a public website

Store raw scraped data in Amazon DocumentDB

Read data from DocumentDB

Transform and clean data:

Remove currency symbols from prices

Convert ratings from text to numeric values

Add ingestion timestamps

Load transformed records into Amazon DynamoDB

Validate data by querying DocumentDB via terminal

ğŸ“‚ Project Structure
aws-nosql-books-pipeline/
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ scrape_to_docdb.py
â”‚   â”œâ”€â”€ transform_and_loadto_dynamodb.py
â”‚   â””â”€â”€ viewdata_in_docdb.py
â”‚
â”œâ”€â”€ docs/
â”‚   â”œâ”€â”€ putty_commands.md
â”‚   â””â”€â”€ documentdb_commands.md
â”‚
â”œâ”€â”€ outputs/
â”‚   â””â”€â”€ screenshots/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ requirements.txt
â””â”€â”€ .gitignore

ğŸ“œ Script Description
1ï¸âƒ£ scrape_to_docdb.py

Scrapes book title, price, stock status, rating, and image URL

Connects securely to Amazon DocumentDB using TLS

Inserts raw book documents into DocumentDB

2ï¸âƒ£ transform_and_loadto_dynamodb.py

Reads raw data from DocumentDB

Cleans and transforms data:

Price â†’ numeric (Decimal)

Rating â†’ integer (1â€“5)

Adds ingestion timestamp

Loads transformed records into Amazon DynamoDB using boto3

3ï¸âƒ£ viewdata_in_docdb.py

Connects to Amazon DocumentDB

Retrieves and displays sample records

Used for data validation and verification

âš™ï¸ How to Run the Project
Prerequisites

Python 3.x

AWS Account

Amazon DocumentDB cluster

Amazon DynamoDB table

AWS credentials configured

TLS certificate (global-bundle.pem)

Install Dependencies
pip install -r requirements.txt

Run the Pipeline
# Step 1: Scrape and store data in DocumentDB
python scrape_to_docdb.py

# Step 2: Transform and load data into DynamoDB
python transform_and_loadto_dynamodb.py

# Step 3: View stored data from DocumentDB
python viewdata_in_docdb.py

ğŸ” Security Note

âš ï¸ Sensitive credentials are not included in this repository.
All secrets such as:

AWS credentials

Database usernames/passwords

TLS certificates

should be stored securely using environment variables or configuration files and excluded via .gitignore.

ğŸ“¸ Results

Successfully scraped book data from the web

Raw data stored in Amazon DocumentDB

Cleaned and transformed data loaded into Amazon DynamoDB

Data verified through terminal-based queries

(Screenshots available in the outputs/screenshots/ directory)

ğŸš€ Future Enhancements

Automate pipeline using AWS Lambda

Add pagination for scraping multiple pages

Implement error logging and retry mechanisms

Add data analytics using Athena or QuickSight

Schedule pipeline with Amazon EventBridge

ğŸ‘©â€ğŸ’» Author

Nikitha (Nikki)
Aspiring Data Engineer | AWS | Python | NoSQL | Data Pipelines
